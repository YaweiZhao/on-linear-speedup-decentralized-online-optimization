\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{8015179Shahram,Kamp:2014:CDO,Koppel-8352032,Zhang2018,pmlr-v70-zhang17g,Xu2015,tcns-7353155,cdc-7798923,acc-7172037,tcns-7479495,Benczur:2018ww,tkde-6311406}
\citation{6760092,tkde-6311406}
\citation{Hazan2016Introduction,ShalevShwartz:2012dz}
\newlabel{sect_introduction}{{1}{1}{}{section.1}{}}
\citation{Hazan2016Introduction,ShalevShwartz:2012dz,introduction-online-optimization}
\citation{8015179Shahram,Kamp:2014:CDO,Koppel-8352032,Zhang2018,pmlr-v70-zhang17g,Xu2015,tcns-7353155,cdc-7798923,acc-7172037,tcns-7479495,Benczur:2018ww,tkde-6311406}
\citation{8015179Shahram}
\citation{8015179Shahram}
\citation{Kamp:2014:CDO}
\citation{Zhang2018}
\citation{Xu2015}
\citation{tcns-7353155}
\citation{cdc-7798923}
\citation{acc-7172037,tcns-7479495}
\citation{6760092}
\citation{tkde-6311406}
\citation{Zinkevich:2003,Hall:2015ct,Hall:2013vr,Jadbabaie:2015wg,Yang:2016ud,Bedi:2018te,Zhang:2016wl,Mokhtari:2016jz,Zhang:2018tu,Gyorgy:2016,NIPS2016_6536,Zhao:2018wx}
\citation{Zinkevich:2003}
\citation{Hall:2015ct,Hall:2013vr}
\citation{Jadbabaie:2015wg,Yang:2016ud,Bedi:2018te,Zhang:2016wl,Mokhtari:2016jz,Zhang:2018tu}
\citation{Gyorgy:2016}
\citation{Gyorgy:2016}
\citation{Zhao:2018wx}
\newlabel{sect_related_work}{{2}{2}{}{section.2}{}}
\citation{Herbster1998,Gyorgy:2005wo,Gyorgy:2012wa,Gyorgy:2016,Mourtada:2017vn,JMLR:v17:13-533,NIPS2016_6536,cesabianchi:hal,pmlr-v84-mohri18a,pmlr-v54-jun17a}
\newlabel{equa_definition_static_regret}{{1}{3}{}{equation.3.1}{}}
\newlabel{equa_definition_our_regret}{{2}{3}{}{equation.3.2}{}}
\newlabel{sec:algorithm}{{4}{4}{}{section.4}{}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{algo_DOG}{{1}{4}{\textsc {DOG}: Decentralized Online Gradient method.\relax }{algorithm.1}{}}
\newlabel{subsection_theoretical_analysis}{{4.2}{4}{}{subsection.4.2}{}}
\newlabel{assumption_bounded_gradient_domain}{{1}{4}{}{Assumption.1}{}}
\citation{Zhao:2018wx}
\citation{Duchi:2012hp,Tang:2018un}
\citation{Duchi:2012hp,Tang:2018un}
\citation{8015179Shahram}
\citation{8015179Shahram}
\citation{pmlr-v70-zhang17g}
\citation{pmlr-v70-zhang17g}
\citation{pmlr-v70-zhang17g}
\newlabel{theorem_regret_upper_bound}{{1}{5}{}{Theorem.1}{}}
\newlabel{corollary_regret_upper_bound}{{1}{5}{}{Corollary.1}{}}
\newlabel{equa_result_Corollary}{{3}{5}{}{equation.4.3}{}}
\newlabel{theorem_local_models_closer}{{2}{5}{}{Theorem.2}{}}
\citation{pmlr-v70-zhang17g}
\citation{tkde-6311406}
\citation{pmlr-v70-zhang17g}
\citation{Katakis:2010:TR}
\newlabel{theorem_implied_other_regret_bound}{{3}{6}{}{Theorem.3}{}}
\newlabel{corollary_implied_other_regret_bound}{{2}{6}{}{Corollary.2}{}}
\newlabel{figure_dynamics}{{1(a)}{6}{Subfigure 1(a)}{subfigure.1.1}{}}
\newlabel{sub@figure_dynamics}{{(a)}{6}{Subfigure 1(a)\relax }{subfigure.1.1}{}}
\newlabel{figure_illus_dynamics}{{1}{6}{An illustration of the dynmaics caused by the time-varying distributions of data. Data distributions $1$ and $2$ satisify $N(1+\sin (t), 1)$ and $N(-1+\sin (t), 1)$, respectively. Suppose we want to conduct classification between data drawn from distributions $1$ and $2$, respectively. The optimal classification model should change over time.\relax }{figure.caption.1}{}}
\bibdata{reference}
\bibcite{JMLR:v17:13-533}{{1}{2016}{{Adamskiy et~al.}}{{Adamskiy, Koolen, Chernov, and Vovk}}}
\newlabel{figure_ave_loss_iteration}{{2(a)}{7}{Subfigure 2(a)}{subfigure.2.1}{}}
\newlabel{sub@figure_ave_loss_iteration}{{(a)}{7}{Subfigure 2(a)\relax }{subfigure.2.1}{}}
\newlabel{figure_ave_loss_iteration_occupancy}{{2(b)}{7}{Subfigure 2(b)}{subfigure.2.2}{}}
\newlabel{sub@figure_ave_loss_iteration_occupancy}{{(b)}{7}{Subfigure 2(b)\relax }{subfigure.2.2}{}}
\newlabel{figure_ave_loss_iteration_usenet2}{{2(c)}{7}{Subfigure 2(c)}{subfigure.2.3}{}}
\newlabel{sub@figure_ave_loss_iteration_usenet2}{{(c)}{7}{Subfigure 2(c)\relax }{subfigure.2.3}{}}
\newlabel{figure_ave_loss_iteration_spam}{{2(d)}{7}{Subfigure 2(d)}{subfigure.2.4}{}}
\newlabel{sub@figure_ave_loss_iteration_spam}{{(d)}{7}{Subfigure 2(d)\relax }{subfigure.2.4}{}}
\newlabel{figure_compare_loss}{{2}{7}{The average loss yielded by DOG is comparable to that yielded by COG.\relax }{figure.caption.2}{}}
\newlabel{table_rho}{{1}{7}{$\rho $ in different topologies used in our experiment. A large $1-\rho $ represents good connectivity of the communication network. ``NC" represents the \textit {No connected} topology, ``FC" represents the \textit {Fully connected} topology, and ``WS" represents the \textit {WattsStrogatz} topology.\relax }{table.caption.4}{}}
\bibcite{tcns-7353155}{{2}{2017}{{Akbari et~al.}}{{Akbari, Gharesifard, and Linder}}}
\bibcite{Bedi:2018te}{{3}{2018}{{Bedi et~al.}}{{Bedi, Sarma, and Rajawat}}}
\bibcite{Benczur:2018ww}{{4}{2018}{{Bencz{\'u}r et~al.}}{{Bencz{\'u}r, Kocsis, and P{\'a}lovics}}}
\bibcite{introduction-online-optimization}{{5}{2011}{{Bubeck}}{{}}}
\bibcite{cesabianchi:hal}{{6}{2012}{{Cesa-Bianchi et~al.}}{{Cesa-Bianchi, Gaillard, Lugosi, and Stoltz}}}
\bibcite{Duchi:2012hp}{{7}{2012}{{{Duchi, John C} et~al.}}{{{Duchi, John C}, {Agarwal, Alekh}, and {Wainwright, Martin J}}}}
\bibcite{Gyorgy:2016}{{8}{2016}{{Gy\"{o}rgy and Szepesv\'{a}ri}}{{}}}
\bibcite{Gyorgy:2005wo}{{9}{2005}{{Gy{\"o}rgy et~al.}}{{Gy{\"o}rgy, Linder, and Lugosi}}}
\bibcite{Gyorgy:2012wa}{{10}{2012}{{Gyorgy et~al.}}{{Gyorgy, Linder, and Lugosi}}}
\bibcite{Hall:2013vr}{{11}{2013}{{Hall and Willett}}{{}}}
\bibcite{Hall:2015ct}{{12}{2015}{{Hall and Willett}}{{}}}
\bibcite{Hazan2016Introduction}{{13}{2016}{{Hazan}}{{}}}
\bibcite{Herbster1998}{{14}{1998}{{Herbster and Warmuth}}{{}}}
\bibcite{6760092}{{15}{2013}{{Hosseini et~al.}}{{Hosseini, Chapman, and Mesbahi}}}
\newlabel{figure_ave_loss_network_size_synthetic}{{3(a)}{8}{Subfigure 3(a)}{subfigure.3.1}{}}
\newlabel{sub@figure_ave_loss_network_size_synthetic}{{(a)}{8}{Subfigure 3(a)\relax }{subfigure.3.1}{}}
\newlabel{figure_ave_loss_network_size_occupancy}{{3(b)}{8}{Subfigure 3(b)}{subfigure.3.2}{}}
\newlabel{sub@figure_ave_loss_network_size_occupancy}{{(b)}{8}{Subfigure 3(b)\relax }{subfigure.3.2}{}}
\newlabel{figure_ave_loss_network_size_usenet2}{{3(c)}{8}{Subfigure 3(c)}{subfigure.3.3}{}}
\newlabel{sub@figure_ave_loss_network_size_usenet2}{{(c)}{8}{Subfigure 3(c)\relax }{subfigure.3.3}{}}
\newlabel{figure_ave_loss_network_size_spam}{{3(d)}{8}{Subfigure 3(d)}{subfigure.3.4}{}}
\newlabel{sub@figure_ave_loss_network_size_spam}{{(d)}{8}{Subfigure 3(d)\relax }{subfigure.3.4}{}}
\newlabel{figure_compare_network_size}{{3}{8}{The average loss yielded by DOG is insensitive to the network size.\relax }{figure.caption.3}{}}
\newlabel{figure_ave_loss_topology_synthetic}{{4(a)}{8}{Subfigure 4(a)}{subfigure.4.1}{}}
\newlabel{sub@figure_ave_loss_topology_synthetic}{{(a)}{8}{Subfigure 4(a)\relax }{subfigure.4.1}{}}
\newlabel{figure_ave_loss_topology_occupancy}{{4(b)}{8}{Subfigure 4(b)}{subfigure.4.2}{}}
\newlabel{sub@figure_ave_loss_topology_occupancy}{{(b)}{8}{Subfigure 4(b)\relax }{subfigure.4.2}{}}
\newlabel{figure_ave_loss_topology_occupancy}{{4(c)}{8}{Subfigure 4(c)}{subfigure.4.3}{}}
\newlabel{sub@figure_ave_loss_topology_occupancy}{{(c)}{8}{Subfigure 4(c)\relax }{subfigure.4.3}{}}
\newlabel{figure_ave_loss_topology_spam}{{4(d)}{8}{Subfigure 4(d)}{subfigure.4.4}{}}
\newlabel{sub@figure_ave_loss_topology_spam}{{(d)}{8}{Subfigure 4(d)\relax }{subfigure.4.4}{}}
\newlabel{figure_compare_topology}{{4}{8}{The average loss yielded by DOG is insensitive to the topology of the network.\relax }{figure.caption.5}{}}
\bibcite{Jadbabaie:2015wg}{{16}{2015}{{Jadbabaie et~al.}}{{Jadbabaie, Rakhlin, Shahrampour, and Sridharan}}}
\bibcite{pmlr-v54-jun17a}{{17}{2017}{{Jun et~al.}}{{Jun, Orabona, Wright, and Willett}}}
\bibcite{Kamp:2014:CDO}{{18}{2014}{{Kamp et~al.}}{{Kamp, Boley, Keren, Schuster, and Sharfman}}}
\bibcite{Katakis:2010:TR}{{19}{2010}{{Katakis et~al.}}{{Katakis, Tsoumakas, and Vlahavas}}}
\bibcite{Koppel-8352032}{{20}{2018}{{Koppel et~al.}}{{Koppel, Paternain, Richard, and Ribeiro}}}
\bibcite{cdc-7798923}{{21}{2016}{{Lee et~al.}}{{Lee, Ribeiro, and Zavlanos}}}
\bibcite{tcns-7479495}{{22}{2018}{{Lee et~al.}}{{Lee, Nedi\IeC {\'c}, and Raginsky}}}
\bibcite{pmlr-v84-mohri18a}{{23}{2018}{{Mohri and Yang}}{{}}}
\bibcite{Mokhtari:2016jz}{{24}{2016}{{Mokhtari et~al.}}{{Mokhtari, Shahrampour, Jadbabaie, and Ribeiro}}}
\bibcite{Mourtada:2017vn}{{25}{2017}{{Mourtada and Maillard}}{{}}}
\bibcite{acc-7172037}{{26}{2015}{{Nedi\IeC {\'c} et~al.}}{{Nedi\IeC {\'c}, Lee, and Raginsky}}}
\bibcite{8015179Shahram}{{27}{2018}{{Shahrampour and Jadbabaie}}{{}}}
\bibcite{ShalevShwartz:2012dz}{{28}{2012}{{Shalev-Shwartz}}{{}}}
\bibcite{Tang:2018un}{{29}{2018}{{Tang et~al.}}{{Tang, Gan, Zhang, Zhang, and Liu}}}
\bibcite{NIPS2016_6536}{{30}{2016}{{Wei et~al.}}{{Wei, Hong, and Lu}}}
\bibcite{Xu2015}{{31}{2015}{{Xu et~al.}}{{Xu, Ling, and Ribeiro}}}
\bibcite{tkde-6311406}{{32}{2013}{{Yan et~al.}}{{Yan, Sundaram, Vishwanathan, and Qi}}}
\bibcite{Yang:2016ud}{{33}{2016}{{Yang et~al.}}{{Yang, Zhang, Jin, and Yi}}}
\bibcite{Zhang2018}{{34}{2018{a}}{{Zhang et~al.}}{{Zhang, Zhao, Hao, Soh, Lee, Miao, and Hoi}}}
\bibcite{Zhang:2016wl}{{35}{2017{a}}{{Zhang et~al.}}{{Zhang, Yang, Yi, Jin, and Zhou}}}
\bibcite{Zhang:2018tu}{{36}{2018{b}}{{Zhang et~al.}}{{Zhang, Yang, rong jin, and Zhou}}}
\bibcite{pmlr-v70-zhang17g}{{37}{2017{b}}{{Zhang et~al.}}{{Zhang, Zhao, Zhu, Hoi, and Zhang}}}
\bibcite{Zhao:2018wx}{{38}{2018}{{Zhao et~al.}}{{Zhao, Qiu, and Liu}}}
\bibcite{Zinkevich:2003}{{39}{2003}{{Zinkevich}}{{}}}
\bibstyle{abbrvnat}
\newlabel{equa_thoerem_update_rule_equivalent}{{4}{13}{Appendix}{equation.6.4}{}}
\newlabel{equa_I3_temp}{{5}{14}{Appendix}{equation.6.5}{}}
\newlabel{Lemma_gradient_norm_bound}{{1}{15}{}{Lemma.1}{}}
\newlabel{equa_Lemma_gradient_norm_temp0}{{7}{16}{Appendix}{equation.6.7}{}}
\newlabel{equa_Lemma_gradient_norm_temp1}{{8}{16}{Appendix}{equation.6.8}{}}
\newlabel{equa_Lemma_gradient_norm_temp2}{{9}{16}{Appendix}{equation.6.9}{}}
\citation{Tang:2018un}
\newlabel{Lemma_average_update_rule}{{2}{17}{}{Lemma.2}{}}
\citation{Tang:2018un}
\citation{8015179Shahram}
\citation{8015179Shahram}
\citation{8015179Shahram}
\citation{8015179Shahram}
\newlabel{Lemma_hanlin_1}{{3}{18}{Lemma $5$ in \citep {Tang:2018un}}{Lemma.3}{}}
\newlabel{Lemma_hanlin_2}{{4}{18}{Lemma $6$ in \citep {Tang:2018un}}{Lemma.4}{}}
\newlabel{theorem_privious_dog_regret}{{4}{18}{Implied by Theorem $3$ and Corollary $4$ in \citet {8015179Shahram}}{Theorem.4}{}}
\newlabel{Lemma_x_variance_norm_square}{{5}{18}{}{Lemma.5}{}}
\newlabel{Lemma_assumption_discussion}{{6}{19}{}{Lemma.6}{}}
