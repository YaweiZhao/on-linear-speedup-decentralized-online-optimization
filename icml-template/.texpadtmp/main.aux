\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{Hazan2016Introduction,ShalevShwartz:2012dz,introduction-online-optimization,Paternain:2016tl,Bach:2016wt,HoNguyen:2017ki,Neely:2017ul,Chen:2018vl,Orabona:2012vr}
\citation{Hazan2016Introduction,ShalevShwartz:2012dz,introduction-online-optimization}
\newlabel{sect_introduction}{{1}{1}{}{section.1}{}}
\citation{Hazan2016Introduction,ShalevShwartz:2012dz,introduction-online-optimization}
\citation{8015179Shahram,Kamp:2014:CDO,Koppel-8352032,Zhang2018,pmlr-v70-zhang17g,Xu2015,tcns-7353155,cdc-7798923,acc-7172037,tcns-7479495,Benczur:2018ww,tkde-6311406}
\citation{8015179Shahram}
\citation{8015179Shahram}
\citation{Kamp:2014:CDO}
\citation{Zhang2018}
\citation{Xu2015}
\citation{tcns-7353155}
\citation{cdc-7798923}
\citation{acc-7172037,tcns-7479495}
\citation{6760092}
\citation{tkde-6311406}
\citation{Zinkevich:2003,Hall:2015ct,Hall:2013vr,Jadbabaie:2015wg,Yang:2016ud,Bedi:2018te,Zhang:2016wl,Mokhtari:2016jz,Zhang:2018tu,Gyorgy:2016,NIPS2016_6536,Zhao:2018wx}
\citation{Zinkevich:2003}
\citation{Hall:2015ct,Hall:2013vr}
\citation{Jadbabaie:2015wg,Yang:2016ud,Bedi:2018te,Zhang:2016wl,Mokhtari:2016jz,Zhang:2018tu}
\citation{Gyorgy:2016}
\citation{Gyorgy:2016}
\citation{Zhao:2018wx}
\newlabel{sect_related_work}{{2}{2}{}{section.2}{}}
\newlabel{equa_definition_previous_regret}{{1}{2}{}{equation.2.1}{}}
\citation{Herbster1998,Gyorgy:2005wo,Gyorgy:2012wa,Gyorgy:2016,Mourtada:2017vn,JMLR:v17:13-533,NIPS2016_6536,cesabianchi:hal,pmlr-v84-mohri18a,pmlr-v54-jun17a}
\newlabel{equa_definition_our_regret}{{2}{3}{}{equation.3.2}{}}
\newlabel{equa_definition_our_regret}{{3}{3}{}{equation.3.3}{}}
\newlabel{sec:algorithm}{{4}{3}{}{section.4}{}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{algo_DOG}{{1}{3}{\textsc {DOG}: Decentralized Online Gradient method.\relax }{algorithm.1}{}}
\newlabel{subsection_theoretical_analysis}{{4.2}{3}{}{subsection.4.2}{}}
\citation{Zhao:2018wx}
\citation{8015179Shahram}
\citation{8015179Shahram}
\citation{8015179Shahram}
\citation{8015179Shahram}
\citation{8015179Shahram}
\citation{8015179Shahram,pmlr-v70-zhang17g,tcns-7353155}
\newlabel{assumption_bounded_gradient_domain}{{1}{4}{}{Assumption.1}{}}
\newlabel{theorem_regret_upper_bound}{{1}{4}{}{Theorem.1}{}}
\newlabel{Corollary_regret_upper_bound}{{1}{4}{}{Corollary.1}{}}
\newlabel{equa_result_Corollary}{{4}{4}{}{equation.4.4}{}}
\newlabel{theorem_privious_dog_regret}{{2}{4}{Implied by Theorem $3$ and Corollary $4$ in \citet {8015179Shahram}}{Theorem.2}{}}
\newlabel{Lemma_x_variance_norm_square}{{1}{4}{}{Lemma.1}{}}
\newlabel{Lemma_assumption_discussion}{{2}{4}{}{Lemma.2}{}}
\citation{Katakis:2010:TR}
\newlabel{figure_dynamics}{{1(a)}{5}{Subfigure 1(a)}{subfigure.1.1}{}}
\newlabel{sub@figure_dynamics}{{(a)}{5}{Subfigure 1(a)\relax }{subfigure.1.1}{}}
\newlabel{figure_illus_dynamics}{{1}{5}{An illustration of the dynmaics caused by the time-varying distributions of data. Data distributions $1$ and $2$ satisify $N(1+\sin (t), 1)$ and $N(-1+\sin (t), 1)$, respectively. Suppose we want to conduct classification between data drawn from distributions $1$ and $2$, respectively. The optimal classification model should change over time.\relax }{figure.caption.1}{}}
\bibdata{reference}
\bibstyle{abbrvnat}
\newlabel{figure_ave_loss_iteration}{{2(a)}{6}{Subfigure 2(a)}{subfigure.2.1}{}}
\newlabel{sub@figure_ave_loss_iteration}{{(a)}{6}{Subfigure 2(a)\relax }{subfigure.2.1}{}}
\newlabel{figure_ave_loss_iteration_occupancy}{{2(b)}{6}{Subfigure 2(b)}{subfigure.2.2}{}}
\newlabel{sub@figure_ave_loss_iteration_occupancy}{{(b)}{6}{Subfigure 2(b)\relax }{subfigure.2.2}{}}
\newlabel{figure_ave_loss_iteration_usenet2}{{2(c)}{6}{Subfigure 2(c)}{subfigure.2.3}{}}
\newlabel{sub@figure_ave_loss_iteration_usenet2}{{(c)}{6}{Subfigure 2(c)\relax }{subfigure.2.3}{}}
\newlabel{figure_ave_loss_iteration_spam}{{2(d)}{6}{Subfigure 2(d)}{subfigure.2.4}{}}
\newlabel{sub@figure_ave_loss_iteration_spam}{{(d)}{6}{Subfigure 2(d)\relax }{subfigure.2.4}{}}
\newlabel{figure_compare_loss}{{2}{6}{The average loss yielded by DOG is comparable to that yielded by COG.\relax }{figure.caption.2}{}}
\newlabel{table_rho}{{1}{6}{$\rho $ in different topologies used in our experiment. ``NC" represents the \textit {No connected} topology, ``FC" represents the \textit {Fully connected} topology, and ``WS" represents the \textit {WattsStrogatz} topology.\relax }{table.caption.4}{}}
\newlabel{figure_ave_loss_network_size_synthetic}{{3(a)}{7}{Subfigure 3(a)}{subfigure.3.1}{}}
\newlabel{sub@figure_ave_loss_network_size_synthetic}{{(a)}{7}{Subfigure 3(a)\relax }{subfigure.3.1}{}}
\newlabel{figure_ave_loss_network_size_occupancy}{{3(b)}{7}{Subfigure 3(b)}{subfigure.3.2}{}}
\newlabel{sub@figure_ave_loss_network_size_occupancy}{{(b)}{7}{Subfigure 3(b)\relax }{subfigure.3.2}{}}
\newlabel{figure_ave_loss_network_size_usenet2}{{3(c)}{7}{Subfigure 3(c)}{subfigure.3.3}{}}
\newlabel{sub@figure_ave_loss_network_size_usenet2}{{(c)}{7}{Subfigure 3(c)\relax }{subfigure.3.3}{}}
\newlabel{figure_ave_loss_network_size_spam}{{3(d)}{7}{Subfigure 3(d)}{subfigure.3.4}{}}
\newlabel{sub@figure_ave_loss_network_size_spam}{{(d)}{7}{Subfigure 3(d)\relax }{subfigure.3.4}{}}
\newlabel{figure_compare_network_size}{{3}{7}{The average loss yielded by DOG is insensitive to the network size.\relax }{figure.caption.3}{}}
\newlabel{figure_ave_loss_topology_synthetic}{{4(a)}{7}{Subfigure 4(a)}{subfigure.4.1}{}}
\newlabel{sub@figure_ave_loss_topology_synthetic}{{(a)}{7}{Subfigure 4(a)\relax }{subfigure.4.1}{}}
\newlabel{figure_ave_loss_topology_occupancy}{{4(b)}{7}{Subfigure 4(b)}{subfigure.4.2}{}}
\newlabel{sub@figure_ave_loss_topology_occupancy}{{(b)}{7}{Subfigure 4(b)\relax }{subfigure.4.2}{}}
\newlabel{figure_ave_loss_topology_occupancy}{{4(c)}{7}{Subfigure 4(c)}{subfigure.4.3}{}}
\newlabel{sub@figure_ave_loss_topology_occupancy}{{(c)}{7}{Subfigure 4(c)\relax }{subfigure.4.3}{}}
\newlabel{figure_ave_loss_topology_spam}{{4(d)}{7}{Subfigure 4(d)}{subfigure.4.4}{}}
\newlabel{sub@figure_ave_loss_topology_spam}{{(d)}{7}{Subfigure 4(d)\relax }{subfigure.4.4}{}}
\newlabel{figure_compare_topology}{{4}{7}{The average loss yielded by DOG is insensitive to the topology of the network.\relax }{figure.caption.5}{}}
\newlabel{equa_thoerem_update_rule_equivalent}{{5}{10}{Appendix}{equation.6.5}{}}
\newlabel{equa_I3_temp}{{6}{11}{Appendix}{equation.6.6}{}}
\newlabel{Lemma_gradient_norm_bound}{{3}{12}{}{Lemma.3}{}}
\newlabel{equa_Lemma_gradient_norm_temp0}{{8}{13}{Appendix}{equation.6.8}{}}
\newlabel{equa_Lemma_gradient_norm_temp1}{{9}{13}{Appendix}{equation.6.9}{}}
\newlabel{equa_Lemma_gradient_norm_temp2}{{10}{13}{Appendix}{equation.6.10}{}}
\citation{Tang:2018un}
\newlabel{Lemma_average_update_rule}{{4}{14}{}{Lemma.4}{}}
\citation{Tang:2018un}
\newlabel{Lemma_hanlin_1}{{5}{15}{Lemma $5$ in \citep {Tang:2018un}}{Lemma.5}{}}
\newlabel{Lemma_hanlin_2}{{6}{15}{Lemma $6$ in \citep {Tang:2018un}}{Lemma.6}{}}
